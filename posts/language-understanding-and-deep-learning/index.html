<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# fb: http://ogp.me/ns/fb# " lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Language Understanding and Deep learning | vanangamudi</title>
<link href="../../assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/colorbox.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/theme.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://vanangamudi.github.io/posts/language-understanding-and-deep-learning/">
<meta property="fb:app_id" content="selvakumar.v2.x">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="vanangamudi">
<link rel="prev" href="../replace-algorithms-with-nn/" title="Replace algorithms with NN" type="text/html">
<meta property="og:site_name" content="vanangamudi">
<meta property="og:title" content="Language Understanding and Deep learning">
<meta property="og:url" content="https://vanangamudi.github.io/posts/language-understanding-and-deep-learning/">
<meta property="og:description" content="[WIP]  Created Monday 02 April 2018 
    
    
    
      
      
        Problems with word-embedding :- 
      
        Word embeddings are flat and do not capture hierarchies. number space vs
     ">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2018-04-02T22:40:30+05:30">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="language understand">
<meta property="article:tag" content="nlp">
<meta property="article:tag" content="nlu">
<meta property="article:tag" content="questions">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://vanangamudi.github.io/">

                <span id="blog-title">vanangamudi</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>
                </li>
<li>
<a href="../../rss.xml">RSS feed</a>
                </li>
<li>
<a href="../../stories/aboutme">About me</a>
                </li>
<li>
<a href="https://twitter.com/paarulakan">Twitter</a>
                </li>
<li>
<a href="https://github.com/vanangamudi">Github</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.src.html" id="sourcelink">Source</a>
    </li>

                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Language Understanding and Deep learning</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    vanangamudi
            </span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2018-04-02T22:40:30+05:30" itemprop="datePublished" title="2018-04-02 22:40">2018-04-02 22:40</time></a></p>
                <p class="commentline">
        
<span class="fb-comments-count" data-url="/posts/language-understanding-and-deep-learning/">


            
        <p class="sourceline"><a href="index.src.html" id="sourcelink">Source</a></p>

        </span></p>
</div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <!--
.. title: Language Understanding and Deep learning
.. slug: language-understanding-and-deep-learning
.. date: 2018-04-02 22:40:30 UTC+05:30
.. tags: deep learning, nlp, language understand, nlu, questions
.. category: NLU, NLP, Deep learning
.. link: 
.. description: 
.. type: text
  -->
  
    <div>[WIP]  Created Monday 02 April 2018 </div>
    <div><br></div>
    <div>
      <p></p>
      <h4>
        <b>Problems with word-embedding :-</b> </h4>
      <p>
        Word embeddings are flat and do not capture hierarchies. number space vs
        word space, for example, the similarity between '8' and 'eight' should
        be captured. Intrinstic and extrinsic properties of words are still
        cannot be captured by word-embeddings alone. Sure we can use character
        embedding and mix/concat them with word-embeddings, but that doesn't
        seem to improve the performance by far. </p>
      <p>
        Meaning of a word is influenced by a lot of factors which can be
        categorised into at least three. <br>
        Syntax - character order, <br>
        Semantics - what are the words that are seen together, <br>
        Pragmatics - we don't know how to handle this so far, because it depends
        upon lot of things like history, culture, contemporary styles of use and
        so on.
      </p>
      <p></p>
      <h4>
        <b>Too deterministic - all the output are probability distributions.</b>
</h4>
      <p>
        The neural network, all it does it mapping between two
        higher-dimensional spaces deterministically. To explain what I mean, I
        will use the &gt;OpAmp vs Microcontroller analogy. In an OpAmp circuitry, the output is
        completly based on the input and the relationship causal. Very similar
        to mechanical devices, it just the medium is different, electrons in
        circuitry. There is no arbitraryness . But I can, make a microcontroller
        to light up an LED regardless of what is on the input side. </p>
      <p>
        I intentionally chose not use the phrase, 'program the microcontroller'.
        Programs is set of rules operating over data structured in, well... a
        structure. No shortcuts. But that is also a curse. Take NLP for example,
        we cannot completely specify language in a set of rules. Although
        link-grammar, dependency-grammar, and recent construction-grammars have
        evolved to more powerful and expressive, NLP is still an unsolved
        problem. </p>
      <p>
        Neural networks tend to perform well on a subset of language
        defined-by/confined-to a text-corpus, because it tries to figure out the
        rules by themselves. But that is not an easy job. It is like watching a
        suspense-thriller. The story is intentionally crafted to make us
        infer/predict something to be true, say the movie, Mulholland Drive is
        about, Diane Selwyn is trying to help Camilla Rhodes figure who was she
        before the car accident, only to break that prediction, like revealing
        that all the damn is a dream, and make us feel like cheated and
        astonished at the same time, so that we are kept invested in the story.
        The movie is actually more complex and there can be more than one
        interpretation other than the one I chose to present here, exactly like
        there are many ways to interpret a sentence. </p>
      <p>
        It is safe to say the that kind of reactions are the after-effects of
        the order in which the events in the story are presented to us. That is
        exactly the reason why beural networks perform well on one dataset and
        not on another and at times not even on the same dataset if the order of
        the samples are shuffled. The reason why our prediction of the hero or
        the anti-hero fails miserably is, we are trying to capture meaningful
        structures of the story from and confined to the events presented, i.e
        from the incomplete information. </p>
      <p>
        We are connecting dots which may or may not be actually connected in the
        story, and we won't know it until we are provided with the full
        information. Similarly the neural networks creates shortcuts and
        believes that is the actual structure of the content in NLP case the
        syntax, semantics and even pragmatics of the language. And believe the
        datasets we currently have cannot capture all of three of them, in their
        entirety. So what the neural networks ends up learning are mere
        shortcuts, unless we provide them all sorts of combinations of meaning
        sentences. </p>
      <p>
        For example to establish the similarity between word 'eight' and number
        '8' we need to provide the neural network enough examples where 'eight'
        and '8' exemplify similar meanings. In context of word-embeddings,
        duplicating samples with 'eight' replaced with '8' and vice-versa should
        capture that similarity, but how to make the neural network understand
        that '1' and '8' are from number spaces and 'one' and 'eight' are from
        word spaces? I don't know the answer to that. (May be multimodal
        learning might help?) </p>
    </div>
  
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/deep-learning/" rel="tag">deep learning</a></li>
            <li><a class="tag p-category" href="../../categories/language-understand/" rel="tag">language understand</a></li>
            <li><a class="tag p-category" href="../../categories/nlp/" rel="tag">nlp</a></li>
            <li><a class="tag p-category" href="../../categories/nlu/" rel="tag">nlu</a></li>
            <li><a class="tag p-category" href="../../categories/questions/" rel="tag">questions</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../replace-algorithms-with-nn/" rel="prev" title="Replace algorithms with NN">Previous post</a>
            </li>
        </ul></nav></aside><section class="comments hidden-print"><h2>Comments</h2>
        
        
<div id="fb-root"></div>
<script>
  window.fbAsyncInit = function() {
    // init the FB JS SDK
    FB.init({
      appId      : 'selvakumar.v2.x',
      status     : true,
      xfbml      : true
    });

  };

  // Load the SDK asynchronously
  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "https://connect.facebook.net/en_US/all.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script><div class="fb-comments" data-href="https://vanangamudi.github.io/posts/language-understanding-and-deep-learning/" data-width="470"></div>


        </section></article><div id="fb-root"></div>
<script>
    // thank lxml for this
    $('.fb-comments-count').each(function(i, obj) {
        var url = obj.attributes['data-url'].value;
        // change here if you dislike the default way of displaying
        // this
        obj.innerHTML = '<fb:comments-count href="' + url + '"></fb:comments-count> comments';
    });

  window.fbAsyncInit = function() {
    // init the FB JS SDK
    FB.init({
      appId      : 'selvakumar.v2.x',
      status     : true,
      xfbml      : true
    });

  };

  // Load the SDK asynchronously
  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "https://connect.facebook.net/en_US/all.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2018         <a href="mailto:selva.developer@gmail.com">vanangamudi</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="../../assets/js/jquery.min.js"></script><script src="../../assets/js/bootstrap.min.js"></script><script src="../../assets/js/moment-with-locales.min.js"></script><script src="../../assets/js/fancydates.js"></script><script src="../../assets/js/jquery.colorbox-min.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates -->
</body>
</html>
